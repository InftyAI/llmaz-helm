{{- if .Values.backendRuntime.enabled -}}
apiVersion: inference.llmaz.io/v1alpha1
kind: BackendRuntime
metadata:
  labels:
    app.kubernetes.io/name: backendruntime
    app.kubernetes.io/part-of: llmaz
    app.kubernetes.io/created-by: llmaz
  name: llamacpp
spec:
  commands:
    - ./llama-server
  image: ghcr.io/ggerganov/llama.cpp
  version: server
  args:
    - mode: Default
      flags:
        - -m
        - "{{`{{ .ModelPath }}`}}"
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
    - mode: SpeculativeDecoding
      flags:
        - -m
        - "{{`{{ .ModelPath }}`}}"
        - -md
        - "{{`{{ .DraftModelPath }}`}}"
        - --host
        - "0.0.0.0"
        - --port
        - "8080"
  resources:
    requests:
      cpu: 2
      memory: 4Gi
    limits:
      cpu: 2
      memory: 4Gi
{{- end }}
